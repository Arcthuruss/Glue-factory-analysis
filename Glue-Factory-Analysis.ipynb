{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# R5.C.08 - Analyse de données : ACP, AFC\n",
    "\n",
    "## 1. Codesource, fichier(s) de données ou un lien de récupération\n",
    "\n",
    "\n",
    "Le code source et le jeu de données sont disponibles sur GitHub :\n",
    "- [https://github.com/Arcthuruss/Glue-factory-analysis](https://github.com/Arcthuruss/Glue-factory-analysis)\n",
    "**Instruction** : Git LFS est requis pour cloner le dépôt car le jeu de données est volumineux (=448Mo).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction, constitution du groupe\n",
    "\n",
    "Le projet consiste à analyser un jeu de données de notre choix en utilisant des techniques d'analyse de données vues lors de la ressource R5.C.08 - Techniques d'intelligence artificielle.\n",
    "Avec pour objectif de pratiquer les techniques d'ACP (Analyse en Composantes Principales) et d'AFC (Analyse Factorielle des Correspondances) sur des variables quantitatives et qualitatives respectivement.\n",
    "\n",
    "Le projet a été réalisé par un groupe de deux personnes :\n",
    "- DONNARD Luc\n",
    "- NÉVOT Pierre\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Description du jeu de données\n",
    "\n",
    "Le jeu de données provient de Kaggle : [https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset](https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset)\n",
    "\n",
    "Il contient plusieurs csv tel que :\n",
    "- 19860105-20210731_laptime.csv qui contient les données des tours de chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_odds.csv qui contient les cotes des chevaux pour chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_race_results.csv qui contient les résultats de chaque course entre 1986 et 2021\n",
    "- 20020615-20210731_corner_passing_order.csv qui contient les positions des chevaux à chaque virage entre 2002 et 2021\n",
    "\n",
    "On a choisi d'utiliser 19860105-20210731_race_results.csv car il contient des informations sur le resultat du cheval gagnant ainsi que pleins d'informations sur la course.\n",
    "Le dataset contient au minimun 1 554 146 entrées et 66 colonnes au total.\n",
    "\n",
    "En voici les principales variables :\n",
    "- Turf and Dirt Category : Catégorie de la piste (herbe ou terre)\n",
    "- Clockwise And Anti-clockwise and Straight Course Category : Catégorie de la course (sens horaire, antihoraire ou ligne droite)\n",
    "- Distance(m) : Distance de la course en mètres\n",
    "- Weather : Conditions météorologiques\n",
    "- Track Condition1 : État de la piste\n",
    "- Final Position : Position finale du cheval\n",
    "- Bracket Number : Numéro de la série\n",
    "- Post Position : Position de départ\n",
    "- Horse Name : Nom du cheval\n",
    "- Age : Âge du cheval\n",
    "- Jockey : Nom du jockey\n",
    "- Total Time(1/10s) : Temps total en dixièmes de seconde\n",
    "- Position 3rd Corner : Position au 3ème virage\n",
    "- Position 4th Corner : Position au 4ème virage\n",
    "- Win Odds(100Yen) : Cote de victoire (en 100 Yen)\n",
    "- Win Fav : Favori à la victoire\n",
    "\n",
    "Le contexte est l'analyse des courses de chevaux au Japon, en utilisant des données historiques pour identifier des tendances et des facteurs influençant les résultats des courses.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Nettoyage de données\n",
    "\n",
    "On a retiré les colonnes non pertinentes qui contenaient des valeurs non pertinentes et incomplètes.\n",
    "On a ensuite traduit les attributs importants du japonais vers l'anglais pour faciliter l'analyse.\n",
    "Tel que le type de course, si la course est dans le sens des aiguilles d'une montre ou non, la météo, l'état de la piste.\n",
    "A part cela le dataset étant propre on a 1 554 146 entrées de données complètes."
   ],
   "id": "df05865cbcbc083c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# imports du projet\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from biplot import *\n",
    "\n",
    "time_str_to_ms = lambda time_as_str : np.array([(int(i[0])*600 + int(i[1])*10 + int(i[2])) for i in [i.replace(':', ' ').replace('.', ' ').split(' ') for i in time_as_str]])"
   ],
   "id": "2363a957283b57c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filter script\n",
    "\n",
    "input_file = \"./datasets/19860105-20210731_race_result.csv\"\n",
    "output_file = \"./clean_datasets/filtered_race_result.csv\"\n",
    "\n",
    "# Ligne qui va être supprimée (pas de nom de colonne car le header est en japonais et prendrais trop de temps)\n",
    "positions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 40, 41, 43, 47, 49, 52, 53, 54, 57, 60, 61, 62, 63, 64, 65]\n",
    "\n",
    "with open(input_file, newline='', encoding='utf-8') as infile, \\\n",
    "    open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        filtered_row = [col for i, col in enumerate(row) if i not in positions]\n",
    "        if all(filtered_row):\n",
    "            writer.writerow(filtered_row)\n",
    "\n",
    "# Remplace le header par le header en anglais\n",
    "with open(output_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "    lines = infile.readlines()\n",
    "header = \"Turf and Dirt Category,Clockwise And Anti-clockwise and Straight Course Category,Distance(m),Weather,Track Condition1,Final Position,Bracket Number,Post Position,Horse Name,Age,Jockey,Total Time(1/10s),Position 3rd Corner,Position 4th Corner,Win Odds(100Yen),Win Fav\"\n",
    "lines[0] = header + \"\\n\"\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    outfile.writelines(lines)\n"
   ],
   "id": "d06f01669cf3a67a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Traduit les noms propres jopnais en anglais\n",
    "\n",
    "# Fonction pour charger le cache\n",
    "def load_cache(cache_file):\n",
    "    try:\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Fonction pour sauvegarder le cache\n",
    "def save_cache(cache, cache_file):\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Si la valeur n'est pas dans le cache, demande la traduction\n",
    "def ask_translation(text):\n",
    "    print(f\"Traduction pour '{text}': \", end=\"\")\n",
    "    return input().strip()\n",
    "\n",
    "def annotate_csv(input_file, output_file, cache_file='translation_cache.json'):\n",
    "    cache = load_cache(cache_file)\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Ne traiter que les 5 premières colonnes\n",
    "    cols_to_check = df.columns[:5]\n",
    "\n",
    "    for col in cols_to_check:\n",
    "        for i, value in enumerate(df[col]):\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "            # Vérifier si la valeur est du texte japonais et non déjà dans le cache\n",
    "            if value not in cache and any('\\u3040' <= char <= '\\u30ff' or '\\u4e00' <= char <= '\\u9faf' for char in str(value)):\n",
    "                translation = ask_translation(value)\n",
    "                cache[value] = translation\n",
    "\n",
    "    save_cache(cache, cache_file)\n",
    "\n",
    "def replace_with_cache(df, cache):\n",
    "    for col in df.columns[:5]:\n",
    "        df[col] = df[col].apply(lambda x: cache.get(x, x) if pd.notna(x) else x)\n",
    "    return df\n",
    "\n",
    "input_file = \"./clean_datasets/filtered_race_result.csv\"\n",
    "output_file = \"./clean_datasets/translated_race_result.csv\"\n",
    "annotate_csv(input_file, output_file)\n",
    "cache = load_cache('translation_cache.json')\n",
    "df = pd.read_csv(input_file)\n",
    "df = replace_with_cache(df, cache)\n",
    "df.to_csv(output_file, index=False)"
   ],
   "id": "f9c190793baf8a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Variables quantitatives : ACP\n",
    "\n",
    "### 5.1 Standardisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e2735bbfd4689634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "translated_file = \"./clean_datasets/translated_race_result.csv\"\n",
    "df = pd.read_csv(translated_file)"
   ],
   "id": "d002e8c970998daa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "distance = df.iloc[:, 2].values\n",
    "final_position = df.iloc[:, 5].values\n",
    "bracket_number = df.iloc[:, 6].values\n",
    "post_position = df.iloc[:, 7].values\n",
    "age = df.iloc[:, 9].values\n",
    "third_corner_position = df.iloc[:, 12].values\n",
    "fourth_corner_position = df.iloc[:, 13].values\n",
    "win_odds = df.iloc[:, 14].values\n",
    "win_fav = df.iloc[:, 15].values\n",
    "time = time_str_to_ms(df.iloc[:, 11].values)\n",
    "\n",
    "values = np.stack(\n",
    "    (\n",
    "        distance,\n",
    "        final_position,\n",
    "        bracket_number,\n",
    "        post_position,\n",
    "        age,\n",
    "        third_corner_position,\n",
    "        fourth_corner_position,\n",
    "        win_odds,\n",
    "        win_fav,\n",
    "        time\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "dimensions = [\n",
    "            \"distance\",\n",
    "            \"final position\",\n",
    "            \"bracket number\",\n",
    "            \"post position\",\n",
    "            \"age\",\n",
    "            \"third corner position\",\n",
    "            \"fourth corner position\",\n",
    "            \"win odds\",\n",
    "            \"win fav\",\n",
    "            \"time\"\n",
    "        ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(values)\n",
    "values_scaled = scaler.transform(values)\n",
    "# values_scaled = values"
   ],
   "id": "4dcf171baa953b91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Entraînement du modèle\n",
    "\n",
    "Effectuez une ACP (Analyse en Composantes Principales) sur les données standardisées."
   ],
   "id": "9cbec7d498c67ec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pca = PCA(n_components=2)\n",
    "pca = PCA()\n",
    "pca_res = pca.fit_transform(values_scaled)"
   ],
   "id": "636a97340f99095b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3 Choix du nombre de composantes principales\n",
    "\n",
    "Expliquez le critère de choix (ex : seuil de variance expliquée)."
   ],
   "id": "b449e2db039913a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c4074b323d648202"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.4 Tableau de valeurs singulières + % variance\n",
    "\n",
    "Présentez un tableau avec les valeurs propres et le pourcentage de variance expliquée par chaque composante."
   ],
   "id": "1357b9f68d26eb48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eig = pd.DataFrame ({\n",
    "    \"Dimension\" :\n",
    "        dimensions,\n",
    "    \"Valeur propre\" : pca.explained_variance_,\n",
    "    \"% valeur propre\" :\n",
    "        np.round (pca.explained_variance_ratio_ * 100),\n",
    "    \"% cum. val. prop.\" :\n",
    "        np.round(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "})\n",
    "print(eig)\n",
    "\n",
    "y1 = list(pca.explained_variance_ratio_)\n",
    "x1 = range(len(y1))\n",
    "plt.bar(x1, y1)\n",
    "plt.show()"
   ],
   "id": "fa8d35ae8ade88a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.5 Visualisation des variables avec biplot, étude sur la corrélation entre variables\n",
    "\n",
    "Affichez un biplot et commentez la corrélation entre variables."
   ],
   "id": "12da155e92cbd0c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def biplot(pca=[],x=None,y=None,components=[0,1],score=None,\\\n",
    "           coeff=None,coeff_labels=None,score_labels=None,circle='T',\\\n",
    "           bigdata=1000,cat=None,cmap=\"viridis\",density=True):\n",
    "    if isinstance(pca,PCA)==True :\n",
    "        coeff = np.transpose(pca.components_[components, :])\n",
    "        score=  pca.fit_transform(x)[:,components]\n",
    "        if isinstance(x,pd.DataFrame)==True :\n",
    "            coeff_labels = list(x.columns)\n",
    "    if score is not None : x = score\n",
    "    if x.shape[1]>1 :\n",
    "        xs = x[:,0]\n",
    "        ys = x[:,1]\n",
    "    else :\n",
    "        xs = x\n",
    "        ys = y\n",
    "    if (len(xs) != len(ys)) : print(\"Warning ! x et y n'ont pas la même taille !\")\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    #x_c = xs * scalex\n",
    "    #y_c = ys * scaley\n",
    "    temp = (xs - xs.min())\n",
    "    x_c = temp / temp.max() * 2 - 1\n",
    "    temp = (ys - ys.min())\n",
    "    y_c = temp / temp.max() * 2 - 1\n",
    "    data = pd.DataFrame({\"x_c\":x_c,\"y_c\":y_c})\n",
    "    print(\"Attention : pour des facilités d'affichage, les données sont centrées-réduites\")\n",
    "    if cat is None : cat = [0]*len(xs)\n",
    "    elif len(pd.Series(cat)) == 1 : cat = list(pd.Series(cat))*len(xs)\n",
    "    elif len(pd.Series(cat)) != len(xs) : print(\"Warning ! Nombre anormal de catégories !\")\n",
    "    cat = pd.Series(cat).astype(\"category\")\n",
    "    fig = plt.figure(figsize=(6,6),facecolor='w')\n",
    "    ax = fig.add_subplot(111)\n",
    "    # Affichage des points\n",
    "    if (len(xs) < bigdata) :\n",
    "        ax.scatter(x_c,y_c, c = cat.cat.codes,cmap=cmap)\n",
    "        if density==True : print(\"Warning ! Le mode density actif n'apparait que si BigData est paramétré.\")\n",
    "    # Affichage des nappes convexes (BigData)\n",
    "    else :\n",
    "        #color\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=(len(np.unique(cat.cat.codes)))) #-(len(np.unique(c)))\n",
    "        cmap = cmap\n",
    "        m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        if density==True :\n",
    "            sns.set_style(\"white\")\n",
    "            sns.kdeplot(x=\"x_c\",y=\"y_c\",data=data)\n",
    "            if len(np.unique(cat)) <= 1 :\n",
    "                sns.kdeplot(x=\"x_c\",y=\"y_c\",data=data, cmap=\"Blues\", shade=True, thresh= 0)\n",
    "            else :\n",
    "                for i in np.unique(cat) :\n",
    "                    color_temp = m.to_rgba(i)\n",
    "                    sns.kdeplot(x=\"x_c\",y=\"y_c\",data=data[cat==i], color=color_temp,\n",
    "                                shade=True, thresh=0.25, alpha=0.25)\n",
    "        for cat_temp in cat.cat.codes.unique() :\n",
    "            x_c_temp = [x_c[i] for i in range(len(x_c)) if (cat.cat.codes[i] == cat_temp)]\n",
    "            y_c_temp = [y_c[i] for i in range(len(y_c)) if (cat.cat.codes[i] == cat_temp)]\n",
    "            points = [ [ None ] * len(x_c_temp) ] * 2\n",
    "            points = np.array(points)\n",
    "            points = points.reshape(len(x_c_temp),2)\n",
    "            points[:,0] = x_c_temp\n",
    "            points[:,1] = y_c_temp\n",
    "            hull = ConvexHull(points)\n",
    "            temp = 0\n",
    "            for simplex in hull.simplices:\n",
    "                color_temp = m.to_rgba(cat_temp)\n",
    "                plt.plot(points[simplex, 0], points[simplex, 1],color=color_temp)\n",
    "                if (temp == 0) :\n",
    "                     plt.xlim(-1,1)\n",
    "                     plt.ylim(-1,1)\n",
    "                     temp = temp+1\n",
    "    if coeff is not None :\n",
    "        if (circle == 'T'):\n",
    "            x_circle = np.linspace(-1, 1, 100)\n",
    "            y_circle = np.linspace(-1, 1, 100)\n",
    "            X, Y = np.meshgrid(x_circle,y_circle)\n",
    "            F = X**2 + Y**2 - 1.0\n",
    "            #fig, ax = plt.subplots()\n",
    "            plt.contour(X,Y,F,[0])\n",
    "        n = coeff.shape[0]\n",
    "        for i in range(n):\n",
    "            plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5,\n",
    "                      head_width=0.05, head_length=0.05)\n",
    "            if coeff_labels is None:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "            else:\n",
    "                plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, coeff_labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "        if score_labels is not None :\n",
    "            for i in range(len(score_labels)) :\n",
    "                temp_x = xs[i] * scalex\n",
    "                temp_y = ys[i] * scaley\n",
    "                plt.text(temp_x,temp_y,list(score_labels)[i])\n",
    "    plt.xlim(-1.2,1.2)\n",
    "    plt.ylim(-1.2,1.2)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.show()"
   ],
   "id": "57e14b9912237e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "biplot(score=pca_res[:,0:2],\n",
    "coeff=np.transpose(pca.components_[0:2,:]),\n",
    "cat=y1[0:1], density=False, coeff_labels = list(range(10))) #dimensions[6:])\n",
    "plt.show()"
   ],
   "id": "5f3c0343baf3502f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.6 Visualisation des individus avec le plan factoriel\n",
    "\n",
    "Projetez les individus sur le plan des deux premières composantes principales."
   ],
   "id": "7b72bbe0e546a6e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pca_df = pd.DataFrame ({\n",
    "    \"Dim1\" : pca_res[:, 8],\n",
    "    \"Dim2\" : pca_res[:, 2]\n",
    "})\n",
    "pca_df.plot.scatter(\"Dim1\", \"Dim2\")\n",
    "plt.xlabel(\"Win Fav (%)\")\n",
    "plt.ylabel(\"Final Position ( %)\")\n",
    "plt.suptitle(\"Premier plan factoriel (%)\")\n",
    "plt.show()"
   ],
   "id": "e433aa3828b27f78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 6. Variables qualitatives : AFC\n",
    "\n",
    "### 6.1 Choix de deux variables, tableau de contingence\n",
    "\n",
    "Je choisit les variables \"Weather\" (météo)\", \"Track Condition1\". Pour analyser la relation entre les conditions météorologiques et l'état de la piste.\n",
    "On a retirer \"Clear\" car c'est une anomalie dans les données qui répresente 37 entrées sur 1 554 146."
   ],
   "id": "2fd620ed7ffb1824"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv('./clean_datasets/translated_race_result.csv')\n",
    "# On garde les colonnes qui ne sont pas Weather et Track Condition1\n",
    "data = data[['Weather', 'Track Condition1']]\n",
    "data = data.dropna()\n",
    "data = data.astype(str)\n",
    "\n",
    "# Tableau de contingence\n",
    "contingency_table = pd.crosstab(data['Weather'], data['Track Condition1'])\n",
    "print(\"Tableau de contingence entre Weather et Track Condition1:\")\n",
    "print(contingency_table)\n",
    "print()\n",
    "\n",
    "# \"Clear\" est une anomalie dans les données donc on la retire\n",
    "contingency_table = contingency_table.drop(index='Clear', errors='ignore')\n",
    "print(\"Tableau de contingence entre Weather et Track Condition1:\")\n",
    "print(contingency_table)"
   ],
   "id": "6cd6c94705ea000b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.2 Test de sphéricité de Bartlett/Test de Chi2, conclusion sur la corrélation",
   "id": "a37c56cd4a878a61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test du Chi2\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, pval, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi2: {chi2}, p-value: {pval}, dof: {dof}, expected: {expected}\")"
   ],
   "id": "9786296f5a78b544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On obtient un p-value très faible (inférieur à 0.05), ce qui indique que nous rejetons l'hypothèse nulle d'indépendance entre les deux variables. On peut donc réaliser une AFC sur les deux variables.",
   "id": "4f0b0fb794a254ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.3 Standardisation",
   "id": "7bf7ab4710b045b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardisation\n",
    "temp = contingency_table.sub(contingency_table.mean())\n",
    "X_scaled = temp.div(contingency_table.std())\n",
    "\n",
    "print(\"Valeurs du tableau de contingence standardisé:\")\n",
    "print(X_scaled)\n",
    "print()\n",
    "\n",
    "X2_val, p_val = calculate_bartlett_sphericity(X_scaled)\n",
    "print(\"Résultats du test de sphéricité de Bartlett:\")\n",
    "print(X2_val, p_val)"
   ],
   "id": "1ca8b032625b0446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La standardisation est importante pour l'AFC car elle permet de mettre les variables sur une échelle comparable, en éliminant les effets de l'échelle et en facilitant l'interprétation des résultats. \n",
    "\n",
    "La p-value du test de sphéricité de Bartlett indique que les variables sont corrélées, ce qui justifie l'utilisation de l'AFC.\n",
    "\n",
    "Dans notre cas le tableau de contingence standardisé montre les écarts par rapport à la moyenne en termes d'écart-type pour chaque combinaison de conditions météorologiques et d'état de la piste. Par exemple, pour les jours de \"Clear Sky\" (ciel clair), on observe des valeurs standardisées positives pour toutes les conditions de piste, indiquant que ces combinaisons sont plus fréquentes que la moyenne. En revanche, pour les jours de \"Light Snow\" (légère neige), les valeurs sont négatives, suggérant que ces combinaisons sont moins fréquentes que la moyenne."
   ],
   "id": "3a3a7ac3149bf337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.4 Entraînement du modèle FactorAnalyzer",
   "id": "4ebfb67f8861247e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Factor Analysis\n",
    "contingency_table.shape\n",
    "n_max_factors = min(contingency_table.shape[0] - 1, contingency_table.shape[1] - 1)\n",
    "print(f\"Nombre maximum de facteurs: {n_max_factors}\")\n",
    "print()\n",
    "\n",
    "fa = FactorAnalyzer(n_factors = n_max_factors, rotation = None)\n",
    "fa.fit(X_scaled)\n",
    "ev, v = fa.get_eigenvalues()\n",
    "fa_res = fa.loadings_\n",
    "print(fa_res)\n",
    "print()\n",
    "\n",
    "fa_new = FactorAnalysis(n_components=n_max_factors, random_state=0)\n",
    "X_transformed = fa_new.fit_transform(X_scaled)\n",
    "print(fa_new.components_)\n",
    "print()\n",
    "\n",
    "print(\"Représentation du premier tableau de facteurs:\")\n",
    "columns = [\"col_\" + str(i + 1) for i in range(n_max_factors)]\n",
    "df_factors = pd.DataFrame(fa.loadings_, columns= columns, index = X_scaled.columns)\n",
    "df_factors.head()"
   ],
   "id": "7111bcae34c4948f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On fait notre étude sur 3 facteurs.\n",
    "\n",
    "Avec le premier tableau, de facteurs, on peut en sortir plusieurs informations.\n",
    "La première colonne montre que la première composante est fortement corrélée avec toutes les conditions de piste, donc on peut supposer que la condition de la piste est lié à la météo.\n",
    "\n",
    "La deuxième colonne montre que la deuxième composante est distincte avec Light Snow et Light Rain, donc on peut supposer que ces deux conditions météorologiques ont un impact différent sur l'état de la piste par rapport aux autres conditions.\n",
    "\n",
    "La troisième colonne montre des valeurs proches de zéro, indiquant que cette composante n'apporte pas d'information significative.\n",
    "\n",
    "Avec le second tableau, de composantes principales, on peut en tirer plusieurs informations.\n",
    "[[ 6.25247509e-01  8.14433079e-01  9.11713071e-01  9.12114932e-01]\n",
    " [-6.31547859e-01  4.02332218e-01 -4.57842846e-02  3.69241284e-02]\n",
    " [ 1.97152911e-01  8.39018263e-02 -1.19613984e-05 -1.36986394e-03]]\n",
    "\n",
    "La première ligne montre que les bonnes conditions de pistes sont associées à un ciel clair donc inversement on peut déduire que les mauvaises conditions de pistes sont associées à des conditions météorologiques défavorables.\n",
    "\n",
    "La deuxième ligne montre qu'il y a un contraste entre les différentes conditions météorologiques, en particulier entre \"Light Snow\" et \"Light Rain\" par rapport aux autres conditions.\n",
    "\n",
    "La troisième ligne montre des valeurs proches de zéro, indiquant que cette composante n'apporte pas d'information significative."
   ],
   "id": "cc2b9da0a40977f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.5 ScreePlot pour les valeurs propres de facteurs",
   "id": "e8abcdab61d0a5ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualisation des facteurs\n",
    "fig, ax = plt.subplots()\n",
    "labels = [str(i+1).zfill(1) for i in range(len(ev))]\n",
    "ax.bar(labels, ev, label = labels)\n",
    "ax.set_xlabel(\"Factors\")\n",
    "ax.set_ylabel(\"Eigenvalues\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.show()"
   ],
   "id": "d935ff17f4f89aaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comme dit précédemment, on peut voir que la première composante explique une grande partie de la variance, la deuxième en explique une partie significative, et la troisième n'apporte pas d'information significative.",
   "id": "7d69add901b9580a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 6.6 Graphiques sur 3 rotations différentes"
   ],
   "id": "83b20ef3dc74775f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "methods = [\n",
    "    (\"FA No rotation\", FactorAnalysis(2,)),\n",
    "    (\"FA Varimax\", FactorAnalysis(2, rotation=\"varimax\")),\n",
    "    (\"FA Quartimax\", FactorAnalysis(2, rotation=\"quartimax\")),\n",
    "]\n",
    "\n",
    "labels_weather = list(X_scaled.index)\n",
    "labels_track = list(X_scaled.columns)\n",
    "\n",
    "# Palette pour la météo\n",
    "n = len(labels_weather)\n",
    "cmap = plt.get_cmap('tab10' if n <= 10 else 'tab20')\n",
    "colors = {lab: cmap(i % cmap.N) for i, lab in enumerate(labels_weather)}\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (method, fa) in zip(axes, methods):\n",
    "    fa.fit(X_scaled)\n",
    "\n",
    "    # Coordonnées des météos (lignes)\n",
    "    transformed_weather = fa.transform(X_scaled)\n",
    "    # Coordonnées des conditions de piste (colonnes)\n",
    "    components = fa.components_\n",
    "\n",
    "    # Afficher les météos\n",
    "    for (x, y, label) in zip(transformed_weather[:, 0], transformed_weather[:, 1], labels_weather):\n",
    "        ax.scatter(x, y, s=100, color=colors[label], edgecolor='k', zorder=3)\n",
    "        ax.text(x + 0.03, y + 0.03, label, fontsize=9, ha=\"center\")\n",
    "\n",
    "    # Afficher les conditions de piste\n",
    "    for i, j, z in zip(components[0, :], components[1, :], labels_track):\n",
    "        ax.arrow(0, 0, i, j, color='black', alpha=0.5, width=0.005, head_width=0.03, zorder=2)\n",
    "        ax.text(i * 1.1, j * 1.1, z, fontsize=10, color='black', ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Habillage du graphique\n",
    "    ax.axhline(0, color='k', linewidth=0.7)\n",
    "    ax.axvline(0, color='k', linewidth=0.7)\n",
    "    ax.set_title(method)\n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel(\"Factor 1\")\n",
    "    ax.set_xlabel(\"Factor 2\")\n",
    "\n",
    "# Légende commune\n",
    "legend_handles = [mlines.Line2D([], [], color=colors[label], marker='o', linestyle='None', markersize=8, markeredgecolor='k', label=label) for label in labels_weather]\n",
    "\n",
    "fig.legend(handles=legend_handles, title=\"Weather\", bbox_to_anchor=(1.02, 0.95), loc='upper left')\n",
    "plt.tight_layout(rect=(0, 0, 0.88, 1))\n",
    "plt.show()\n"
   ],
   "id": "335be77c86732e78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Questions du TD2\n",
    "\n",
    "##### Quelles sont les variables corrélées ?\n",
    "\n",
    "Les variables corrélées identifiées à partir de l'analyse factorielle comprennent :\n",
    "- Entre \"Good\" et \"Bad\" - Corrélation négative ce qui veut dire que lorsque la piste est en bonne condition, elle n'est pas en mauvaise condition et vice versa.\n",
    "- Entre \"Heavy\" et \"Slightly Heavy\" - Corrélation positive ce qui veut dire que lorsque la piste est en condition \"Heavy\", elle est aussi en condition \"Slightly Heavy\" et inversement.\n",
    "- Entre \"Good\" et \"Clear Sky\" / \"Light Snow\" - Corrélation positive ce qui veut dire que lorsque la piste est en bonne condition, le temps est soit clair soit avec de la légère neige et inversement.\n",
    "- Entre \"Bad\" / \"Heavy\" / \"Slightly Heavy\" et \"Rain\" / \"Snow\" / \"Cloudy\" - Corrélation positive ce qui veut dire que lorsque la piste est en mauvaise condition, le temps est soit pluvieux, neigeux ou nuageux et inversement.\n",
    "- Entre \"Heavy\" / \"Slightly Heavy\" et \"Snow\" / \"Light Snow\" - Corrélation positive ce qui veut dire que lorsque la piste est en condition \"Heavy\" ou \"Slightly Heavy\", le temps est soit neigeux ou avec de la légère neige et inversement. (La piste atteint une dégradation intermédiaire)\n",
    "\n",
    "##### A partir de ces corrélations, pourriez-vous trouver le sens de chaque facteur ?\n",
    "- Le premier facteur semble représenter une dimension de \"Condition de la piste\", où des valeurs élevées indiquent des conditions de piste plus favorables (Good, Slightly Heavy) et des valeurs faibles indiquent des conditions moins favorables (Bad, Heavy).\n",
    "- Ainsi que de représenter la qualité de la piste en fonction de la météo.\n",
    "\n",
    "- Le deuxième facteur semble représenter une dimension de \"Type de dégradation de la piste\", où on distingue les pistes \"Slightly Heavy\" des pistes \"Heavy\" ou \"Bad\". Comme un contraste entre les conditions intermédiaires et extrêmes."
   ],
   "id": "a07e429d6455a24c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "### Résultats de l'AFC (NEVOT Pierre)\n",
    "\n",
    "Les résultats obtenues de l'AFC que la météo a un impact sur l'état de la piste, les jours de beau temps tel que \"Clear Sky\" et \"Light Snow\" sont associés à de bonnes conditions de piste (\"Good\" et \"Slightly Heavy\"), tandis que les jours de mauvais temps comme \"Rain\", \"Snow\" et \"Cloudy\" sont liés à des conditions de piste défavorables (\"Bad\" et \"Heavy\"). La neige légère semble être une exception, étant associée à la fois à de bonnes et à des conditions de piste modérément dégradées (\"Slightly Heavy\").\n",
    "\n",
    "### Résultats de l'ACP (DONNARD Luc)\n",
    "\n"
   ],
   "id": "35b5ef5b9c956fb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

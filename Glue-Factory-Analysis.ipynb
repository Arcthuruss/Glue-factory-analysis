{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2c857c4594de0a",
   "metadata": {},
   "source": [
    "# R5.C.08 - Analyse de données : ACP, AFC\n",
    "\n",
    "## 1. Codesource, fichier(s) de données ou un lien de récupération\n",
    "\n",
    "\n",
    "Le code source et le jeu de données sont disponibles sur GitHub :\n",
    "- [https://github.com/Arcthuruss/Glue-factory-analysis](https://github.com/Arcthuruss/Glue-factory-analysis)\n",
    "**Instruction** : Git LFS est requis pour cloner le dépôt car le jeu de données est volumineux (=448Mo).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction, constitution du groupe\n",
    "\n",
    "Le projet consiste à analyser un jeu de données de notre choix en utilisant des techniques d'analyse de données vues lors de la ressource R5.C.08 - Techniques d'intelligence artificielle.\n",
    "Avec pour objectif de pratiquer les techniques d'ACP (Analyse en Composantes Principales) et d'AFC (Analyse Factorielle des Correspondances) sur des variables quantitatives et qualitatives respectivement.\n",
    "\n",
    "Le projet a été réalisé par un groupe de deux personnes :\n",
    "- DONNARD Luc\n",
    "- NÉVOT Pierre\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Description du jeu de données\n",
    "\n",
    "Le jeu de données provient de Kaggle : [https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset](https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset)\n",
    "\n",
    "Il contient plusieurs csv tel que :\n",
    "- 19860105-20210731_laptime.csv qui contient les données des tours de chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_odds.csv qui contient les cotes des chevaux pour chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_race_results.csv qui contient les résultats de chaque course entre 1986 et 2021\n",
    "- 20020615-20210731_corner_passing_order.csv qui contient les positions des chevaux à chaque virage entre 2002 et 2021\n",
    "\n",
    "On a choisi d'utiliser 19860105-20210731_race_results.csv car il contient des informations sur le resultat du cheval gagnant ainsi que pleins d'informations sur la course.\n",
    "Le dataset contient au minimun 1 554 146 entrées et 66 colonnes au total.\n",
    "\n",
    "En voici les principales variables :\n",
    "- Turf and Dirt Category : Catégorie de la piste (herbe ou terre)\n",
    "- Clockwise And Anti-clockwise and Straight Course Category : Catégorie de la course (sens horaire, antihoraire ou ligne droite)\n",
    "- Distance(m) : Distance de la course en mètres\n",
    "- Weather : Conditions météorologiques\n",
    "- Track Condition1 : État de la piste\n",
    "- Final Position : Position finale du cheval\n",
    "- Bracket Number : Numéro de la série\n",
    "- Post Position : Position de départ\n",
    "- Horse Name : Nom du cheval\n",
    "- Age : Âge du cheval\n",
    "- Jockey : Nom du jockey\n",
    "- Total Time(1/10s) : Temps total en dixièmes de seconde\n",
    "- Position 3rd Corner : Position au 3ème virage\n",
    "- Position 4th Corner : Position au 4ème virage\n",
    "- Win Odds(100Yen) : Cote de victoire (en 100 Yen)\n",
    "- Win Fav : Favori à la victoire\n",
    "\n",
    "Le contexte est l'analyse des courses de chevaux au Japon, en utilisant des données historiques pour identifier des tendances et des facteurs influençant les résultats des courses.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Nettoyage de données\n",
    "\n",
    "On a retiré les colonnes non pertinentes qui contenaient des valeurs non pertinentes et incomplètes.\n",
    "On a ensuite traduit les attributs importants du japonais vers l'anglais pour faciliter l'analyse.\n",
    "Tel que le type de course, si la course est dans le sens des aiguilles d'une montre ou non, la météo, l'état de la piste.\n",
    "A part cela le dataset étant propre on a 1 554 146 entrées de données complètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552ec585b383641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:15:00.871449Z",
     "start_time": "2025-10-08T20:14:14.900028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Traduit les noms propres jopnais en anglais\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction pour charger le cache\n",
    "def load_cache(cache_file):\n",
    "    try:\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Fonction pour sauvegarder le cache\n",
    "def save_cache(cache, cache_file):\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Si la valeur n'est pas dans le cache, demande la traduction\n",
    "def ask_translation(text):\n",
    "    print(f\"Traduction pour '{text}': \", end=\"\")\n",
    "    return input().strip()\n",
    "\n",
    "def annotate_csv(input_file, output_file, cache_file='translation_cache.json'):\n",
    "    cache = load_cache(cache_file)\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Ne traiter que les 5 premières colonnes\n",
    "    cols_to_check = df.columns[:5]\n",
    "\n",
    "    for col in cols_to_check:\n",
    "        for i, value in enumerate(df[csudo ubuntu-driversol]):\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "            # Vérifier si la valeur est du texte japonais et non déjà dans le cache\n",
    "            if value not in cache and any('\\u3040' <= char <= '\\u30ff' or '\\u4e00' <= char <= '\\u9faf' for char in str(value)):\n",
    "                translation = ask_translation(value)\n",
    "                cache[value] = translation\n",
    "\n",
    "    save_cache(cache, cache_file)\n",
    "\n",
    "def replace_with_cache(df, cache):\n",
    "    for col in df.columns[:5]:\n",
    "        df[col] = df[col].apply(lambda x: cache.get(x, x) if pd.notna(x) else x)\n",
    "    return df\n",
    "\n",
    "input_file = \"./clean_datasets/filtered_race_result.csv\"\n",
    "output_file = \"./clean_datasets/translated_race_result.csv\"\n",
    "annotate_csv(input_file, output_file)\n",
    "cache = load_cache('translation_cache.json')\n",
    "df = pd.read_csv(input_file)\n",
    "df = replace_with_cache(df, cache)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869d1cee1e267d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Variables quantitatives : ACP\n",
    "\n",
    "### 5.1 Standardisation\n",
    "\n",
    "Expliquez pourquoi la standardisation est nécessaire et appliquez-la sur les variables quantitatives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add066a2a91c776b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5450a2d6f2f0efe3",
   "metadata": {},
   "source": [
    "### 5.2 Entraînement du modèle\n",
    "\n",
    "Effectuez une ACP (Analyse en Composantes Principales) sur les données standardisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d59e8504cc28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e1e0e99aa42a6b9",
   "metadata": {},
   "source": [
    "### 5.3 Choix du nombre de composantes principales\n",
    "\n",
    "Expliquez le critère de choix (ex : seuil de variance expliquée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee86f090a5bc682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8649f85bac127ca9",
   "metadata": {},
   "source": [
    "### 5.4 Tableau de valeurs singulières + % variance\n",
    "\n",
    "Présentez un tableau avec les valeurs propres et le pourcentage de variance expliquée par chaque composante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87039d9584da1d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ff2cd7e5675640",
   "metadata": {},
   "source": [
    "### 5.5 Visualisation des variables avec biplot, étude sur la corrélation entre variables\n",
    "\n",
    "Affichez un biplot et commentez la corrélation entre variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365793d932d1b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebba25b217bf264",
   "metadata": {},
   "source": [
    "### 5.6 Visualisation des individus avec le plan factoriel\n",
    "\n",
    "Projetez les individus sur le plan des deux premières composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27332e9d0698fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f56fce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Variables qualitatives : AFC\n",
    "\n",
    "### 6.1 Choix de deux variables, tableau de contingence\n",
    "\n",
    "Sélectionnez deux variables qualitatives et construisez leur tableau de contingence.\n",
    "\n",
    "Je choisit les variables \"Weather\" (météo)\", \"Track Condition1\". Pour analyser la relation entre les conditions météorologiques et l'état de la piste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26e7117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tableau de contingence entre Weather et Track Condition1:\n",
      "Track Condition1    Bad    Good  Heavy  Slightly Heavy\n",
      "Weather                                               \n",
      "Clear                16      21      0               0\n",
      "Clear Sky         16080  777420  52203           96163\n",
      "Cloudy            32916  299726  49105           80919\n",
      "Light Rain         8020   25520   8021           13511\n",
      "Light Snow            9     683     41             499\n",
      "Rain              28278   21666  17161           24619\n",
      "Snow                341     736    143             329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv('./clean_datasets/translated_race_result.csv')\n",
    "# On garde les colonnes qui ne sont pas Weather et Track Condition1\n",
    "data = data[['Weather', 'Track Condition1']]\n",
    "data = data.dropna()\n",
    "data = data.astype(str)\n",
    "\n",
    "# Tableau de contingence\n",
    "contingency_table = pd.crosstab(data['Weather'], data['Track Condition1'])\n",
    "print(\"Tableau de contingence entre Weather et Track Condition1:\")\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34755cb7",
   "metadata": {},
   "source": [
    "### 6.2 Test de sphéricité de Bartlett/Test de Chi2, conclusion sur la corrélation\n",
    "\n",
    "Effectuez le test du Chi2 sur le tableau de contingence et concluez sur la corrélation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b103951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track Condition1    Bad    Good  Heavy  Slightly Heavy\n",
      "Weather                                               \n",
      "Clear                16      21      0               0\n",
      "Clear Sky         16080  777420  52203           96163\n",
      "Cloudy            32916  299726  49105           80919\n",
      "Light Rain         8020   25520   8021           13511\n",
      "Light Snow            9     683     41             499\n",
      "Rain              28278   21666  17161           24619\n",
      "Snow                341     736    143             329\n"
     ]
    }
   ],
   "source": [
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08de8f3",
   "metadata": {},
   "source": [
    "### 6.3 Standardisation\n",
    "\n",
    "Expliquez la standardisation pour l'AFC/AMC si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38916e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf431c0",
   "metadata": {},
   "source": [
    "### 6.4 Entraînement du modèle FactorAnalyzer\n",
    "\n",
    "Entraînez un modèle FactorAnalyzer sur les données qualitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f97ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e524399d",
   "metadata": {},
   "source": [
    "### 6.5 ScreePlot pour les valeurs propres de facteurs\n",
    "\n",
    "Affichez le scree plot des valeurs propres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6447b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdacff8d",
   "metadata": {},
   "source": [
    "\n",
    "### 6.6 Graphiques sur 3 rotations différentes\n",
    "\n",
    "Visualisez les résultats de l'analyse factorielle avec trois types de rotations différentes (ex : varimax, quartimax, equamax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594291a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b9a294",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (glue-env)",
   "language": "python",
   "name": "glue-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# R5.C.08 - Analyse de données : ACP, AFC\n",
    "\n",
    "## 1. Codesource, fichier(s) de données ou un lien de récupération\n",
    "\n",
    "\n",
    "Le code source et le jeu de données sont disponibles sur GitHub :\n",
    "- [https://github.com/Arcthuruss/Glue-factory-analysis](https://github.com/Arcthuruss/Glue-factory-analysis)\n",
    "**Instruction** : Git LFS est requis pour cloner le dépôt car le jeu de données est volumineux (=448Mo).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introduction, constitution du groupe\n",
    "\n",
    "Le projet consiste à analyser un jeu de données de notre choix en utilisant des techniques d'analyse de données vues lors de la ressource R5.C.08 - Techniques d'intelligence artificielle.\n",
    "Avec pour objectif de pratiquer les techniques d'ACP (Analyse en Composantes Principales) et d'AFC (Analyse Factorielle des Correspondances) sur des variables quantitatives et qualitatives respectivement.\n",
    "\n",
    "Le projet a été réalisé par un groupe de deux personnes :\n",
    "- DONNARD Luc\n",
    "- NÉVOT Pierre\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Description du jeu de données\n",
    "\n",
    "Le jeu de données provient de Kaggle : [https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset](https://www.kaggle.com/datasets/takamotoki/jra-horse-racing-dataset)\n",
    "\n",
    "Il contient plusieurs csv tel que :\n",
    "- 19860105-20210731_laptime.csv qui contient les données des tours de chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_odds.csv qui contient les cotes des chevaux pour chaque course entre 1986 et 2021\n",
    "- 19860105-20210731_race_results.csv qui contient les résultats de chaque course entre 1986 et 2021\n",
    "- 20020615-20210731_corner_passing_order.csv qui contient les positions des chevaux à chaque virage entre 2002 et 2021\n",
    "\n",
    "On a choisi d'utiliser 19860105-20210731_race_results.csv car il contient des informations sur le resultat du cheval gagnant ainsi que pleins d'informations sur la course.\n",
    "Le dataset contient au minimun 1 554 146 entrées et 66 colonnes au total.\n",
    "\n",
    "En voici les principales variables :\n",
    "- Turf and Dirt Category : Catégorie de la piste (herbe ou terre)\n",
    "- Clockwise And Anti-clockwise and Straight Course Category : Catégorie de la course (sens horaire, antihoraire ou ligne droite)\n",
    "- Distance(m) : Distance de la course en mètres\n",
    "- Weather : Conditions météorologiques\n",
    "- Track Condition1 : État de la piste\n",
    "- Final Position : Position finale du cheval\n",
    "- Bracket Number : Numéro de la série\n",
    "- Post Position : Position de départ\n",
    "- Horse Name : Nom du cheval\n",
    "- Age : Âge du cheval\n",
    "- Jockey : Nom du jockey\n",
    "- Total Time(1/10s) : Temps total en dixièmes de seconde\n",
    "- Position 3rd Corner : Position au 3ème virage\n",
    "- Position 4th Corner : Position au 4ème virage\n",
    "- Win Odds(100Yen) : Cote de victoire (en 100 Yen)\n",
    "- Win Fav : Favori à la victoire\n",
    "\n",
    "Le contexte est l'analyse des courses de chevaux au Japon, en utilisant des données historiques pour identifier des tendances et des facteurs influençant les résultats des courses.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Nettoyage de données\n",
    "\n",
    "On a retiré les colonnes non pertinentes qui contenaient des valeurs non pertinentes et incomplètes.\n",
    "On a ensuite traduit les attributs importants du japonais vers l'anglais pour faciliter l'analyse.\n",
    "Tel que le type de course, si la course est dans le sens des aiguilles d'une montre ou non, la météo, l'état de la piste.\n",
    "A part cela le dataset étant propre on a 1 554 146 entrées de données complètes."
   ],
   "id": "bb2c857c4594de0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T07:27:58.501771Z",
     "start_time": "2025-10-10T07:27:58.022382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports du projet\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "time_str_to_ms = lambda time_as_str : [(int(i[0])*600 + int(i[1])*10 + int(i[2])) for i in [i.replace(':', ' ').replace('.', ' ').split(' ') for i in time_as_str]]"
   ],
   "id": "369ceed8a077120a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filter script\n",
    "\n",
    "input_file = \"./datasets/19860105-20210731_race_result.csv\"\n",
    "output_file = \"./clean_datasets/filtered_race_result.csv\"\n",
    "\n",
    "# Ligne qui va être supprimée (pas de nom de colonne car le header est en japonais et prendrais trop de temps)\n",
    "positions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 36, 40, 41, 43, 47, 49, 52, 53, 54, 57, 60, 61, 62, 63, 64, 65]\n",
    "\n",
    "with open(input_file, newline='', encoding='utf-8') as infile, \\\n",
    "    open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in reader:\n",
    "        filtered_row = [col for i, col in enumerate(row) if i not in positions]\n",
    "        if all(filtered_row):\n",
    "            writer.writerow(filtered_row)\n",
    "\n",
    "# Remplace le header par le header en anglais\n",
    "with open(output_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "    lines = infile.readlines()\n",
    "header = \"Turf and Dirt Category,Clockwise And Anti-clockwise and Straight Course Category,Distance(m),Weather,Track Condition1,Final Position,Bracket Number,Post Position,Horse Name,Age,Jockey,Total Time(1/10s),Position 3rd Corner,Position 4th Corner,Win Odds(100Yen),Win Fav\"\n",
    "lines[0] = header + \"\\n\"\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "    outfile.writelines(lines)\n"
   ],
   "id": "e38a3cc95b82f5f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T20:15:00.871449Z",
     "start_time": "2025-10-08T20:14:14.900028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Traduit les noms propres jopnais en anglais\n",
    "\n",
    "# Fonction pour charger le cache\n",
    "def load_cache(cache_file):\n",
    "    try:\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "# Fonction pour sauvegarder le cache\n",
    "def save_cache(cache, cache_file):\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Si la valeur n'est pas dans le cache, demande la traduction\n",
    "def ask_translation(text):\n",
    "    print(f\"Traduction pour '{text}': \", end=\"\")\n",
    "    return input().strip()\n",
    "\n",
    "def annotate_csv(input_file, output_file, cache_file='translation_cache.json'):\n",
    "    cache = load_cache(cache_file)\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Ne traiter que les 5 premières colonnes\n",
    "    cols_to_check = df.columns[:5]\n",
    "\n",
    "    for col in cols_to_check:\n",
    "        for i, value in enumerate(df[col]):\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "            # Vérifier si la valeur est du texte japonais et non déjà dans le cache\n",
    "            if value not in cache and any('\\u3040' <= char <= '\\u30ff' or '\\u4e00' <= char <= '\\u9faf' for char in str(value)):\n",
    "                translation = ask_translation(value)\n",
    "                cache[value] = translation\n",
    "\n",
    "    save_cache(cache, cache_file)\n",
    "\n",
    "def replace_with_cache(df, cache):\n",
    "    for col in df.columns[:5]:\n",
    "        df[col] = df[col].apply(lambda x: cache.get(x, x) if pd.notna(x) else x)\n",
    "    return df\n",
    "\n",
    "input_file = \"./clean_datasets/filtered_race_result.csv\"\n",
    "output_file = \"./clean_datasets/translated_race_result.csv\"\n",
    "annotate_csv(input_file, output_file)\n",
    "cache = load_cache('translation_cache.json')\n",
    "df = pd.read_csv(input_file)\n",
    "df = replace_with_cache(df, cache)\n",
    "df.to_csv(output_file, index=False)"
   ],
   "id": "d552ec585b383641",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T07:17:29.882409Z",
     "start_time": "2025-10-10T07:17:24.519011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translated_file = \"./clean_datasets/translated_race_result.csv\"\n",
    "df = pd.read_csv(translated_file)"
   ],
   "id": "ed7624904e10371f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Variables quantitatives : ACP\n",
    "\n",
    "### 5.1 Standardisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "e869d1cee1e267d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T08:43:34.117264Z",
     "start_time": "2025-10-10T08:43:34.110383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "distance = df.iloc[:, 2].values\n",
    "final_position = df.iloc[:, 5].values\n",
    "bracket_number = df.iloc[:, 6].values\n",
    "post_position = df.iloc[:, 7].values\n",
    "age = df.iloc[:, 9].values\n",
    "third_corner_position = df.iloc[:, 12].values\n",
    "fourth_corner_position = df.iloc[:, 13].values\n",
    "win_odds = df.iloc[:, 14].values\n",
    "win_fav = df.iloc[:, 15].values\n"
   ],
   "id": "add066a2a91c776b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.2 Entraînement du modèle\n",
    "\n",
    "Effectuez une ACP (Analyse en Composantes Principales) sur les données standardisées."
   ],
   "id": "5450a2d6f2f0efe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T08:43:58.054620Z",
     "start_time": "2025-10-10T08:43:57.127505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pca = PCA(n_components=2)\n",
    "print(pca.fit_transform(final_position))"
   ],
   "id": "d07d59e8504cc28e",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 1.  2.  3. ... 14. 15. 16.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m pca = PCA(n_components=\u001B[32m2\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mpca\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfinal_position\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    318\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    319\u001B[39m         return_tuple = (\n\u001B[32m    320\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    321\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    322\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:466\u001B[39m, in \u001B[36mPCA.fit_transform\u001B[39m\u001B[34m(self, X, y)\u001B[39m\n\u001B[32m    443\u001B[39m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    444\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    445\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001B[39;00m\n\u001B[32m    446\u001B[39m \n\u001B[32m    447\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    464\u001B[39m \u001B[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001B[39;00m\n\u001B[32m    465\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     U, S, _, X, x_is_centered, xp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m U \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    468\u001B[39m         U = U[:, : \u001B[38;5;28mself\u001B[39m.n_components_]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:503\u001B[39m, in \u001B[36mPCA._fit\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    493\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    494\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPCA with svd_solver=\u001B[39m\u001B[33m'\u001B[39m\u001B[33marpack\u001B[39m\u001B[33m'\u001B[39m\u001B[33m is not supported for Array API inputs.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    495\u001B[39m     )\n\u001B[32m    497\u001B[39m \u001B[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001B[39;00m\n\u001B[32m    498\u001B[39m \u001B[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001B[39;00m\n\u001B[32m    499\u001B[39m \u001B[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001B[39;00m\n\u001B[32m    500\u001B[39m \u001B[38;5;66;03m# the input data contrary to the other solvers.\u001B[39;00m\n\u001B[32m    501\u001B[39m \u001B[38;5;66;03m# The copy will happen\u001B[39;00m\n\u001B[32m    502\u001B[39m \u001B[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m503\u001B[39m X = \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    504\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    505\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    506\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    507\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    508\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsc\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    509\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    510\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    511\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    512\u001B[39m \u001B[38;5;28mself\u001B[39m._fit_svd_solver = \u001B[38;5;28mself\u001B[39m.svd_solver\n\u001B[32m    513\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fit_svd_solver == \u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m issparse(X):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2952\u001B[39m         out = X, y\n\u001B[32m   2953\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m-> \u001B[39m\u001B[32m2954\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2955\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m   2956\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Cours/projet/glue-factory-analysis/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1091\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1084\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1085\u001B[39m             msg = (\n\u001B[32m   1086\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33marray=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00marray\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1087\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1088\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1089\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mif it contains a single sample.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1090\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m1091\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1093\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(array.dtype, \u001B[33m\"\u001B[39m\u001B[33mkind\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m array.dtype.kind \u001B[38;5;129;01min\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mUSV\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1094\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1095\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mdtype=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mnumeric\u001B[39m\u001B[33m'\u001B[39m\u001B[33m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1096\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1097\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Expected 2D array, got 1D array instead:\narray=[ 1.  2.  3. ... 14. 15. 16.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.3 Choix du nombre de composantes principales\n",
    "\n",
    "Expliquez le critère de choix (ex : seuil de variance expliquée)."
   ],
   "id": "8e1e0e99aa42a6b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7ee86f090a5bc682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.4 Tableau de valeurs singulières + % variance\n",
    "\n",
    "Présentez un tableau avec les valeurs propres et le pourcentage de variance expliquée par chaque composante."
   ],
   "id": "8649f85bac127ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87039d9584da1d7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.5 Visualisation des variables avec biplot, étude sur la corrélation entre variables\n",
    "\n",
    "Affichez un biplot et commentez la corrélation entre variables."
   ],
   "id": "83ff2cd7e5675640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f365793d932d1b45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.6 Visualisation des individus avec le plan factoriel\n",
    "\n",
    "Projetez les individus sur le plan des deux premières composantes principales."
   ],
   "id": "ebba25b217bf264"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b27332e9d0698fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 6. Variables qualitatives : AFC\n",
    "\n",
    "### 6.1 Choix de deux variables, tableau de contingence\n",
    "\n",
    "Sélectionnez deux variables qualitatives et construisez leur tableau de contingence.\n",
    "\n",
    "### 6.2 Test de sphéricité de Bartlett/Test de Chi2, conclusion sur la corrélation\n",
    "\n",
    "Effectuez le test du Chi2 sur le tableau de contingence et concluez sur la corrélation.\n",
    "\n",
    "### 6.3 Standardisation\n",
    "\n",
    "Expliquez la standardisation pour l'AFC/AMC si nécessaire.\n",
    "\n",
    "### 6.4 Entraînement du modèle FactorAnalyzer\n",
    "\n",
    "Entraînez un modèle FactorAnalyzer sur les données qualitatives.\n",
    "\n",
    "### 6.5 ScreePlot pour les valeurs propres de facteurs\n",
    "\n",
    "Affichez le scree plot des valeurs propres.\n",
    "\n",
    "### 6.6 Graphiques sur 3 rotations différentes\n",
    "\n",
    "Visualisez les résultats de l'analyse factorielle avec trois types de rotations différentes (ex : varimax, quartimax, equamax).\n",
    "\n",
    "---"
   ],
   "id": "1f4e8db902605f2f"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
